{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "jB_8sDNlAZxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP8YSVAdENe5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import gc\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "1AFUEvS8AdEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading of the datasets of 10 nodes\n",
        "number_nodes = 10\n",
        "train_loader_10 = np.loadtxt(\"tsp-data/tsp10_train_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))\n",
        "val_loader_10 = np.loadtxt(\"tsp-data/tsp10_val_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))\n",
        "test_loader_10 = np.loadtxt(\"tsp-data/tsp10_test_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))"
      ],
      "metadata": {
        "id": "ZKT6OewyZomW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting between point coordinates and path\n",
        "train_input_10 = train_loader_10[:,:number_nodes*2]\n",
        "train_input_10 = train_input_10.reshape(train_input_10.shape[0], number_nodes, 2)\n",
        "train_input_10 = torch.Tensor(train_input_10)\n",
        "train_path_10 = train_loader_10[:,number_nodes*2:]\n",
        "\n",
        "val_input_10 = val_loader_10[:,:number_nodes*2]\n",
        "val_input_10 = val_input_10.reshape(val_input_10.shape[0], number_nodes, 2)\n",
        "val_input_10 = torch.Tensor(val_input_10)\n",
        "val_path_10 = val_loader_10[:,number_nodes*2:]\n",
        "\n",
        "test_input_10 = test_loader_10[:,:number_nodes*2]\n",
        "test_input_10 = test_input_10.reshape(test_input_10.shape[0], number_nodes, 2)\n",
        "test_input_10 = torch.Tensor(test_input_10)\n",
        "test_path_10 = test_loader_10[:,number_nodes*2:]"
      ],
      "metadata": {
        "id": "1hZWslkXMvss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8HiYdwk8Ghw"
      },
      "outputs": [],
      "source": [
        "#Loading of the datasets of 20 nodes\n",
        "number_nodes = 20\n",
        "train_loader_20 = np.loadtxt(\"tsp-data/tsp20_train_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))\n",
        "val_loader_20 = np.loadtxt(\"tsp-data/tsp20_val_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))\n",
        "test_loader_20 = np.loadtxt(\"tsp-data/tsp20_test_concorde.txt\", usecols=np.concatenate((np.arange(0,number_nodes*2),np.arange(number_nodes*2+1,number_nodes*3+2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCkB7Deqi4RY"
      },
      "outputs": [],
      "source": [
        "# Splitting between point coordinates and path\n",
        "train_input_20 = train_loader_20[:,:number_nodes*2]\n",
        "train_input_20 = train_input_20.reshape(train_input_20.shape[0], number_nodes, 2)\n",
        "train_input_20 = torch.Tensor(train_input_20)\n",
        "train_path_20 = train_loader_20[:,number_nodes*2:]\n",
        "\n",
        "val_input_20 = val_loader_20[:,:number_nodes*2]\n",
        "val_input_20 = val_input_20.reshape(val_input_20.shape[0], number_nodes, 2)\n",
        "val_input_20 = torch.Tensor(val_input_20)\n",
        "val_path_20 = val_loader_20[:,number_nodes*2:]\n",
        "\n",
        "test_input_20 = test_loader_20[:,:number_nodes*2]\n",
        "test_input_20 = test_input_20.reshape(test_input_20.shape[0], number_nodes, 2)\n",
        "test_input_20 = torch.Tensor(test_input_20)\n",
        "test_path_20 = test_loader_20[:,number_nodes*2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of the model"
      ],
      "metadata": {
        "id": "i2fwIM7cDiOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7uy6FGCDOfd"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, h, k, is_gpu):\n",
        "    super(Embedding, self).__init__()\n",
        "    #Parameter\n",
        "    self.k = k\n",
        "    #Linear layer for node embedding\n",
        "    self.linear_node = nn.Linear(2, h, bias = True)\n",
        "    #Linear layers for edge embedding\n",
        "    self.linear_distance = nn.Linear(1, h//2, bias = True)\n",
        "    self.linear_delta = nn.Linear(1, h//2, bias = False)\n",
        "    self.is_gpu = is_gpu\n",
        "  \n",
        "  def forward(self, nodes):\n",
        "    #Computation of distance matrix\n",
        "    distances = (((nodes[:,None,:,:] - nodes[:,:,None,:])**2).sum(axis=3)).sqrt()\n",
        "    if self.is_gpu :     \n",
        "      distances = distances.cuda()\n",
        "\n",
        "    #Computation of delta matrix\n",
        "    delta = torch.zeros(distances.shape)\n",
        "    if self.is_gpu :\n",
        "      delta = delta.cuda()\n",
        "    self_connection = np.arange(distances.shape[1])\n",
        "    indices = torch.argsort(distances, axis = 2)[:,:,0:self.k+1]\n",
        "    delta = delta.scatter(2, indices, 1)\n",
        "    delta[:,self_connection,self_connection] += 1\n",
        "\n",
        "    #Node embedding\n",
        "    emb_nodes = self.linear_node(nodes)\n",
        "\n",
        "    #Edge embedding\n",
        "    emb_distances = self.linear_distance(distances[:,:,:,None])\n",
        "    emb_delta = self.linear_delta(delta[:,:,:,None])\n",
        "    emb_edges = torch.cat((emb_distances, emb_delta), 3)\n",
        "\n",
        "    return emb_nodes, emb_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z56kw8hKkHl-"
      },
      "outputs": [],
      "source": [
        "class Graph_conv_layer(nn.Module):\n",
        "  def __init__(self, h, epsilon, is_gpu):\n",
        "    super(Graph_conv_layer, self).__init__()\n",
        "    self.is_gpu = is_gpu\n",
        "    #Parameter\n",
        "    self.epsilon = epsilon\n",
        "    #Linear layers\n",
        "    self.linear_W = []\n",
        "    for i in range(5):\n",
        "      l = nn.Linear(h, h, bias = False)\n",
        "      if self.is_gpu :\n",
        "        l.cuda()\n",
        "      self.linear_W.append(l)\n",
        "    self.linear_W = nn.ModuleList(self.linear_W)\n",
        "    #Non-linearities\n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #Batch normalizations\n",
        "    self.batch_norm_nodes = nn.BatchNorm1d(h)\n",
        "    if self.is_gpu :\n",
        "      self.batch_norm_nodes.cuda()\n",
        "    self.batch_norm_edges = nn.BatchNorm2d(h)\n",
        "    if self.is_gpu :\n",
        "      self.batch_norm_edges.cuda()\n",
        "\n",
        "  def forward(self, emb_nodes, emb_edges):\n",
        "    #Computation of (emb_nodes)^(l+1) which is called new_emb_nodes in the following\n",
        "    #Linear layers\n",
        "    linear_nodes_1 = self.linear_W[0](emb_nodes)\n",
        "    linear_nodes_2 = self.linear_W[1](emb_nodes)\n",
        "    #Computation of eta\n",
        "    sigmoid_edges = self.sigmoid(emb_edges)\n",
        "    eta = sigmoid_edges / (sigmoid_edges.sum(dim=2)[:,:,None,:]+self.epsilon)\n",
        "    #Sum of the two terms\n",
        "    sum_nodes = linear_nodes_1 + (eta * linear_nodes_2[:,:,None,:]).sum(dim=2)\n",
        "    #Batch normalization\n",
        "    sum_nodes_bn = self.batch_norm_nodes(sum_nodes.permute((0,2,1))).permute((0,2,1))\n",
        "    #Computation of new_emb_nodes\n",
        "    new_emb_nodes = emb_nodes + self.relu(sum_nodes_bn)\n",
        "\n",
        "    #Computation of (emb_edges)^(l+1) which is called new_emb_edges in the following\n",
        "    #Linear layers\n",
        "    linear_edges = self.linear_W[2](emb_edges)\n",
        "    linear_nodes_1 = self.linear_W[3](emb_nodes)\n",
        "    linear_nodes_2 = self.linear_W[4](emb_nodes)\n",
        "    #Sum of the three terms\n",
        "    sum_edges = linear_edges + linear_nodes_1[:,None,:,:] + linear_nodes_2[:,:,None,:]\n",
        "    #Batch normalization\n",
        "    sum_edges_bn = self.batch_norm_edges(sum_edges.permute((0,3,1,2))).permute((0,2,3,1))\n",
        "    #Computation of new_emb_edges\n",
        "    new_emb_edges = emb_edges + self.relu(sum_edges_bn)\n",
        "\n",
        "    return new_emb_nodes, new_emb_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn51yHVnOKzT"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, dim_layers, is_gpu):\n",
        "    super(MLP, self).__init__()\n",
        "    self.is_gpu = is_gpu\n",
        "    #Parameter\n",
        "    self.nb_layers = len(dim_layers)\n",
        "    #Linear layers\n",
        "    self.linears = []\n",
        "    for i in range(self.nb_layers):\n",
        "      l = nn.Linear(dim_layers[i][0], dim_layers[i][1], bias = True)\n",
        "      if self.is_gpu :\n",
        "        l.cuda()\n",
        "      self.linears.append(l)\n",
        "    self.linears = nn.ModuleList(self.linears)\n",
        "    #Non-linearity\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, emb_edges):\n",
        "    #Initialization\n",
        "    res = emb_edges\n",
        "\n",
        "    #Linear layer + non-linearity for each layer except the last one\n",
        "    for i in range(self.nb_layers-1):\n",
        "      res = self.linears[i](res)\n",
        "      res = self.relu(res)\n",
        "    \n",
        "    #Last linear layer (no non-linearity)\n",
        "    res = self.linears[self.nb_layers-1](res)\n",
        "    \n",
        "    return(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK_rGlZyOMuY"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, h, k, dim_hidden_mlp, nb_graph_layers, epsilon, is_gpu):\n",
        "    super(Net, self).__init__()\n",
        "    #Parameter\n",
        "    self.nb_graph_layers = nb_graph_layers\n",
        "    #Computation of dim_layers_mlp\n",
        "    dim_layers_mlp = [[h, dim_hidden_mlp[0]]]\n",
        "    for i in range(len(dim_hidden_mlp)-1):\n",
        "      dim_layers_mlp.append([dim_hidden_mlp[i], dim_hidden_mlp[i+1]])\n",
        "    dim_layers_mlp.append([dim_hidden_mlp[len(dim_hidden_mlp)-1], 2])\n",
        "    #Input layer\n",
        "    self.embedding = Embedding(h, k, is_gpu)\n",
        "    #Graph convolution layers\n",
        "    self.graph_layers = nn.ModuleList()\n",
        "    for i in range(self.nb_graph_layers):\n",
        "      self.graph_layers.append(Graph_conv_layer(h,epsilon, is_gpu))\n",
        "    #MLP\n",
        "    self.mlp = MLP(dim_layers_mlp, is_gpu)\n",
        "  \n",
        "\n",
        "  def forward(self, input):\n",
        "    #Input layer\n",
        "    emb_nodes, emb_edges = self.embedding(input)\n",
        "\n",
        "    #Graph convolution layers\n",
        "    for i in range(self.nb_graph_layers):\n",
        "      emb_nodes, emb_edges = self.graph_layers[i](emb_nodes, emb_edges)\n",
        "\n",
        "    #MLP\n",
        "    output = self.mlp(emb_edges)\n",
        "    \n",
        "    return(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful functions"
      ],
      "metadata": {
        "id": "pl9tBI0kDw0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7kVgsyhjGvl"
      },
      "outputs": [],
      "source": [
        "def adjacency(path):\n",
        "  # Returns the adjacency matrix of a path from the dataset\n",
        "  path = torch.tensor(path)\n",
        "  heads = path.int()\n",
        "  tails = torch.roll(heads,-1)\n",
        "  adjacency_matrix = np.zeros((len(path),len(path)))\n",
        "  adjacency_matrix[heads,tails] = 1\n",
        "  adjacency_matrix = torch.tensor(adjacency_matrix, dtype = torch.int64)\n",
        "  return adjacency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvXWpYPmjRfe"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix(path):\n",
        "  # Returns the adjacency matrix of a path which is the output of the network\n",
        "  return(adjacency(path[:-1]-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyzAqXzOeygS"
      },
      "outputs": [],
      "source": [
        "def dist(nodes, path, target):\n",
        "  # Returns the distance of the path for these nodes \n",
        "  # The path has not the same format if it is coming from the dataset (target=True) or from the network output (target=False)\n",
        "  if target:\n",
        "    adj_matrix = torch.Tensor(np.apply_along_axis(adjacency, 1, path))\n",
        "  else:\n",
        "    adj_matrix = torch.Tensor(np.apply_along_axis(adjacency_matrix, 1, path))\n",
        "  distances = (((nodes[:,None,:,:] - nodes[:,:,None,:])**2).sum(axis=3)).sqrt()\n",
        "  return (adj_matrix * distances).sum(axis=(1,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beamsearch"
      ],
      "metadata": {
        "id": "jzZIEEGmG1oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTW5J-6yU2_D"
      },
      "outputs": [],
      "source": [
        "def beamsearch_batch(probabilities, w_b, random=True):\n",
        "  # Beamsearch algorithm applied on the probabilities with a beam width of w_b \n",
        "  batch,nb_nodes,_ = probabilities.shape\n",
        "  a = np.ones((batch,w_b))\n",
        "  index = -1*np.ones((batch,w_b, nb_nodes))\n",
        "  if random:\n",
        "    init = np.random.randint(0, nb_nodes, (batch,w_b))\n",
        "  else:\n",
        "    init = np.zeros((batch,w_b), dtype=np.int8)\n",
        "  np.put_along_axis(index, init[:,:,None], 0, 2)\n",
        "  for i in range(nb_nodes-1):\n",
        "    prob = a[:,:,None]*np.take_along_axis(probabilities,np.argmax(index==i, axis=2)[:,:,None], axis=1)*(index==-1)\n",
        "    argsort_prob = np.argsort(prob.reshape((batch,w_b*nb_nodes)), axis=1)\n",
        "    sort_index = np.take_along_axis(index, (argsort_prob//nb_nodes)[:,:,None], axis=1)[:,-w_b:]\n",
        "    np.put_along_axis(sort_index, (argsort_prob%nb_nodes)[:,-w_b:,None], i+1, 2)\n",
        "    index = sort_index\n",
        "    a = np.sort(prob.reshape((batch,w_b*nb_nodes)), axis=1)[:,-w_b:]\n",
        "  return np.argsort(index, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimality gap"
      ],
      "metadata": {
        "id": "StFYhAv7G8-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hVPEer8kcn7"
      },
      "outputs": [],
      "source": [
        "def optimality_gap_greedy(val_input,val_output, val_path):\n",
        "  # Returns the optimality gap with the greedy method applied on val_path\n",
        "  prob = np.exp(val_output.detach().numpy())\n",
        "  beam_search = beamsearch_batch(prob[:,:,:,1],1,False)\n",
        "  d1 = dist(val_input, beam_search[:,0,:], True)\n",
        "  d2 = dist(val_input, val_path, False)\n",
        "  return((d1/d2-1).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya1fozm4gY0k"
      },
      "outputs": [],
      "source": [
        "def optimality_gap_beamsearch(val_input,val_output, val_path):\n",
        "  # Returns the optimality gap with the beamsearch method applied on val_path\n",
        "  prob = np.exp(val_output.detach().numpy())\n",
        "  beam_search = beamsearch_batch(prob[:,:,:,1],1280)\n",
        "  d1 = dist(val_input, beam_search[:,-1,:], True)\n",
        "  d2 = dist(val_input, val_path, False)\n",
        "  return((d1/d2-1).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgJSJ1egknK7"
      },
      "outputs": [],
      "source": [
        "def optimality_gap_beamsearch_shortest(val_input,val_output, val_path):\n",
        "  # Returns the optimality gap with the beamsearch + shortest path method applied on val_path\n",
        "  prob = np.exp(val_output.detach().numpy())\n",
        "  beam_search = beamsearch_batch(prob[:,:,:,1],1280)\n",
        "  beam_search = beam_search.transpose((0,2,1))\n",
        "  b_shape = beam_search.shape\n",
        "  beam_search = beam_search.reshape((b_shape[0]*b_shape[1], b_shape[2]))\n",
        "  def dist_1D(b_search):\n",
        "    b_search = b_search.reshape((b_shape[0], b_shape[1]))\n",
        "    return(dist(val_input, b_search, True))\n",
        "  A = np.apply_along_axis(dist_1D, 0, beam_search)\n",
        "  d1 = np.min(A, axis=1)\n",
        "  d2 = dist(val_input, val_path, False)\n",
        "  return((d1/d2-1).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on one epoch"
      ],
      "metadata": {
        "id": "HA_4Gw0bGRK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, input, target, optimizer, epoch, n):\n",
        "  network.train()\n",
        "  weight = torch.Tensor([ n*n/(n*n-2*n)/2, n*n/(2*n)/2])\n",
        "  loss_function = nn.NLLLoss(weight.cuda())\n",
        "  loss_function.cuda()\n",
        "    \n",
        "  list_loss = []\n",
        "\n",
        "  for batch in range(500):\n",
        "    input_batch = input[ batch*20 : (batch+1)*20 ]\n",
        "    input_batch = input_batch.cuda()\n",
        "    \n",
        "    target_batch = target[ batch*20 : (batch+1)*20 ]\n",
        "    target_batch = torch.flatten(target_batch, start_dim=0, end_dim=2)\n",
        "    target_batch = target_batch.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_batch = network(input_batch)\n",
        "\n",
        "    output_batch = F.log_softmax(output_batch, dim=3)\n",
        "    output_batch = torch.flatten(output_batch, start_dim=0, end_dim=2)\n",
        "    loss = loss_function(output_batch, target_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    list_loss.append(loss)\n",
        "  with open('training_loss.txt','a') as f_training_loss:\n",
        "    print(torch.tensor(list_loss).mean(), file=f_training_loss)\n",
        "  gc.collect()  \n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mYCYk40yEytE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VcOpwFe8fWC"
      },
      "outputs": [],
      "source": [
        "def train_multi(network, input_10, target_10, input_20, target_20, optimizer, epoch):\n",
        "  network.train()\n",
        "  n=10\n",
        "  weight_10 = torch.Tensor([ n*n/(n*n-2*n)/2, n*n/(2*n)/2])\n",
        "  loss_function_10 = nn.NLLLoss(weight_10.cuda())\n",
        "  loss_function_10.cuda()\n",
        "\n",
        "  n=20\n",
        "  weight_20 = torch.Tensor([ n*n/(n*n-2*n)/2, n*n/(2*n)/2])\n",
        "  loss_function_20 = nn.NLLLoss(weight_20.cuda())\n",
        "  loss_function_20.cuda()\n",
        "    \n",
        "  list_loss = []\n",
        "\n",
        "  for batch in range(250):\n",
        "    input_batch = input_10[ batch*20 : (batch+1)*20 ]\n",
        "    input_batch = input_batch.cuda()\n",
        "    \n",
        "    target_batch = target_10[ batch*20 : (batch+1)*20 ]\n",
        "    target_batch = torch.flatten(target_batch, start_dim=0, end_dim=2)\n",
        "    target_batch = target_batch.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_batch = network(input_batch)\n",
        "\n",
        "    output_batch = F.log_softmax(output_batch, dim=3)\n",
        "    output_batch = torch.flatten(output_batch, start_dim=0, end_dim=2)\n",
        "    loss = loss_function_10(output_batch, target_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    list_loss.append(loss)\n",
        "\n",
        "    input_batch = input_20[ batch*20 : (batch+1)*20 ]\n",
        "    input_batch = input_batch.cuda()\n",
        "    \n",
        "    target_batch = target_20[ batch*20 : (batch+1)*20 ]\n",
        "    target_batch = torch.flatten(target_batch, start_dim=0, end_dim=2)\n",
        "    target_batch = target_batch.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_batch = network(input_batch)\n",
        "\n",
        "    output_batch = F.log_softmax(output_batch, dim=3)\n",
        "    output_batch = torch.flatten(output_batch, start_dim=0, end_dim=2)\n",
        "    loss = loss_function_20(output_batch, target_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    list_loss.append(loss)\n",
        "  with open('training_lossMulti.txt','a') as f_training_loss:\n",
        "    print(torch.tensor(list_loss).mean(), file=f_training_loss)\n",
        "  gc.collect()  \n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on one dataset"
      ],
      "metadata": {
        "id": "NVvzvHHDIItR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters \n",
        "nb_nodes = 10\n",
        "nb_epoch = 1000\n",
        "h = 300\n",
        "k = 20\n",
        "dim_hidden_mlp = [300, 300]\n",
        "nb_graph_layers = 30\n",
        "epsilon = 1e-20\n",
        "lr = 0.05\n",
        "\n",
        "# Data\n",
        "train_input = train_input_10\n",
        "train_path = train_path_10\n",
        "val_input = val_input_10\n",
        "val_path = val_path_10\n",
        "\n",
        "# Initialization of the network\n",
        "network = Net(h, k, dim_hidden_mlp, nb_graph_layers, epsilon, True)\n",
        "network.cuda()\n",
        "\n",
        "# Initialization of the optimizer and loss function\n",
        "optimizer = optim.Adam(network.parameters(), lr)\n",
        "loss = 1\n",
        "weight = torch.Tensor([ nb_nodes*nb_nodes/(nb_nodes*nb_nodes-2*nb_nodes)/2, nb_nodes*nb_nodes/(2*nb_nodes)/2])\n",
        "loss_function = nn.NLLLoss(weight)\n",
        "\n",
        "# Computation of the adjacency matrix of the validation dataset\n",
        "val_target = torch.zeros((len(val_path), nb_nodes, nb_nodes))\n",
        "val_path = torch.Tensor(val_path)\n",
        "for i in range(len(val_path)):\n",
        "    val_target[i] = adjacency_matrix(val_path[i])\n",
        "val_target = val_target.type(torch.LongTensor)\n",
        "\n",
        "# Initialization of the validation model (on the cpu because of RAM problem)\n",
        "model_val = Net(h, k, dim_hidden_mlp, nb_graph_layers, epsilon, False)\n",
        "\n",
        "with open('logs/greedy.txt', 'w') as f_greedy:\n",
        "      with open('logs/beam.txt', 'w') as f_beam:\n",
        "        with open('logs/shortest.txt', 'w') as f_shortest:\n",
        "            with open('logs/loss.txt','w') as f_loss:\n",
        "                for epoch in range(nb_epoch):\n",
        "                    # Computation of the sub-selection of the training dataset used and of its adjacency matrix\n",
        "                    sub_selection = np.arange(len(train_path))\n",
        "                    random.shuffle(sub_selection)\n",
        "                    sub_selection = sub_selection[:10000]\n",
        "\n",
        "                    input_batch = torch.Tensor(train_input[sub_selection])\n",
        "                    path_batch = torch.Tensor(train_path[sub_selection])\n",
        "\n",
        "                    target_batch = torch.zeros((len(path_batch), nb_nodes, nb_nodes))\n",
        "                    for i in range(len(path_batch)):\n",
        "                      target_batch[i] = adjacency_matrix(path_batch[i])\n",
        "                    target_batch = target_batch.type(torch.LongTensor)\n",
        "\n",
        "                    # Training\n",
        "                    train(network, input_batch, target_batch, optimizer, epoch, nb_nodes)\n",
        "\n",
        "                    # Evaluation every 5 epochs \n",
        "                    if epoch%5 == 0:\n",
        "                        # Initialization \n",
        "                        greedy = []\n",
        "                        beam = []\n",
        "                        shortest = []\n",
        "                        losses = []\n",
        "\n",
        "                        # Sub-selection of a part of the validation dataset\n",
        "                        sub_selection = np.arange(len(val_path))\n",
        "                        random.shuffle(sub_selection)\n",
        "                        sub_selection = sub_selection[:500]\n",
        "                        val_input_batch = torch.Tensor(val_input[sub_selection])\n",
        "                        val_path_batch = torch.Tensor(val_path[sub_selection])\n",
        "                        val_target_batch = torch.Tensor(val_target[sub_selection])\n",
        "                        val_target_batch = torch.flatten(val_target_batch, start_dim=0, end_dim=2)\n",
        "\n",
        "                        # Loading of the current model into the cpu ones (because of RAM problem)\n",
        "                        model_val.load_state_dict(network.state_dict())\n",
        "\n",
        "                        for batch in range(2):\n",
        "                          # Evaluation \n",
        "                          val_output_cpu = model_val(val_input_batch[100*batch:100*(batch+1)])\n",
        "                          val_output_cpu = F.log_softmax(val_output_cpu, dim=3)\n",
        "\n",
        "                          #Computation of the optimalities gap and of the loss\n",
        "                          greedy.append(optimality_gap_greedy(val_input_batch[100*batch:100*(batch+1)],val_output_cpu, val_path_batch[100*batch:100*(batch+1)]))\n",
        "                          beam.append(optimality_gap_beamsearch(val_input_batch[100*batch:100*(batch+1)],val_output_cpu, val_path_batch[100*batch:100*(batch+1)]))\n",
        "                          shortest.append(optimality_gap_beamsearch_shortest(val_input_batch[100*batch:100*(batch+1)],val_output_cpu, val_path_batch[100*batch:100*(batch+1)]))\n",
        "                          val_output_cpu = torch.flatten(val_output_cpu, start_dim=0, end_dim=2)\n",
        "                          losses.append(loss_function(val_output_cpu, val_target_batch[100*batch*nb_nodes*nb_nodes:100*(batch+1)*nb_nodes*nb_nodes]))\n",
        "\n",
        "                          gc.collect()  \n",
        "                          torch.cuda.empty_cache()\n",
        "\n",
        "                        # Saving of the optimalities gap and of the loss into files\n",
        "                        print(torch.tensor(greedy).mean(), file=f_greedy, flush=True)\n",
        "                        print(torch.tensor(beam).mean(), file=f_beam, flush=True)\n",
        "                        print(torch.tensor(shortest).mean(), file=f_shortest, flush=True)\n",
        "                        new_loss = torch.tensor(losses).mean()\n",
        "                        print(new_loss, file=f_loss, flush=True)\n",
        "\n",
        "                        # Decreasing of the learning rate if the loss does not sufficiently decrease \n",
        "                        if (loss - new_loss) / loss < 0.01:\n",
        "                          lr /=1.01\n",
        "                          print('new lr : ',lr)\n",
        "                          optimizer = optim.Adam(network.parameters(), lr)\n",
        "                        loss = new_loss\n",
        "\n",
        "                    # Saving of the model every 20 epochs\n",
        "                    if (epoch%20 == 0):\n",
        "                        torch.save(network.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "RhRQhi6Y-NOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on two datasets"
      ],
      "metadata": {
        "id": "0WSBQw9_-pWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters \n",
        "nb_epoch = 1000\n",
        "h = 300\n",
        "k = 20\n",
        "dim_hidden_mlp = [300, 300]\n",
        "nb_graph_layers = 30\n",
        "epsilon = 1e-20\n",
        "lr = 0.001\n",
        "\n",
        "# Initialization of the network\n",
        "network = Net(h, k, dim_hidden_mlp, nb_graph_layers, epsilon, True)\n",
        "network.cuda()\n",
        "\n",
        "# Initialization of the optimizer and loss function\n",
        "optimizer = optim.Adam(network.parameters(), lr)\n",
        "loss = 1\n",
        "nb_nodes = 10\n",
        "weight_10 = torch.Tensor([ nb_nodes*nb_nodes/(nb_nodes*nb_nodes-2*nb_nodes)/2, nb_nodes*nb_nodes/(2*nb_nodes)/2])\n",
        "loss_function_10 = nn.NLLLoss(weight_10)\n",
        "nb_nodes = 20\n",
        "weight_20 = torch.Tensor([ nb_nodes*nb_nodes/(nb_nodes*nb_nodes-2*nb_nodes)/2, nb_nodes*nb_nodes/(2*nb_nodes)/2])\n",
        "loss_function_20 = nn.NLLLoss(weight_20)\n",
        "\n",
        "# Computation of the adjacency matrix of the two validation datasets\n",
        "val_target_10 = torch.zeros((len(val_path_10), 10, 10))\n",
        "val_path_10 = torch.Tensor(val_path_10)\n",
        "for i in range(len(val_path_10)):\n",
        "    val_target_10[i] = adjacency_matrix(val_path_10[i])\n",
        "val_target_10 = val_target_10.type(torch.LongTensor)\n",
        "val_target_20 = torch.zeros((len(val_path_20), 20, 20))\n",
        "val_path_20 = torch.Tensor(val_path_20)\n",
        "for i in range(len(val_path_20)):\n",
        "    val_target_20[i] = adjacency_matrix(val_path_20[i])\n",
        "val_target_20 = val_target_20.type(torch.LongTensor)\n",
        "\n",
        "# Initialization of the validation model (on the cpu because of RAM problem)\n",
        "model_val = Net(h, k, dim_hidden_mlp, nb_graph_layers, epsilon, False)\n",
        "\n",
        "with open('logs/greedyMulti.txt', 'w') as f_greedy:\n",
        "      with open('logs/beamMulti.txt', 'w') as f_beam:\n",
        "        with open('logs/shortestMulti.txt', 'w') as f_shortest:\n",
        "            with open('logs/lossMulti.txt','w') as f_loss:\n",
        "                for epoch in range(nb_epoch):\n",
        "                    # Computation of the sub-selections of the training datasets used and of their adjacency matrix\n",
        "                    sub_selection = np.arange(len(train_path_10))\n",
        "                    random.shuffle(sub_selection)\n",
        "                    sub_selection = sub_selection[:5000]\n",
        "\n",
        "                    input_batch_10 = torch.Tensor(train_input_10[sub_selection])\n",
        "                    path_batch_10 = torch.Tensor(train_path_10[sub_selection])\n",
        "\n",
        "                    target_batch_10 = torch.zeros((len(path_batch_10), 10, 10))\n",
        "                    for i in range(len(path_batch_10)):\n",
        "                      target_batch_10[i] = adjacency_matrix(path_batch_10[i])\n",
        "                    target_batch_10 = target_batch_10.type(torch.LongTensor)\n",
        "\n",
        "                    sub_selection = np.arange(len(train_path_20))\n",
        "                    random.shuffle(sub_selection)\n",
        "                    sub_selection = sub_selection[:5000]\n",
        "\n",
        "                    input_batch_20 = torch.Tensor(train_input_20[sub_selection])\n",
        "                    path_batch_20 = torch.Tensor(train_path_20[sub_selection])\n",
        "\n",
        "                    target_batch_20 = torch.zeros((len(path_batch_20), 20, 20))\n",
        "                    for i in range(len(path_batch_20)):\n",
        "                      target_batch_20[i] = adjacency_matrix(path_batch_20[i])\n",
        "                    target_batch_20 = target_batch_20.type(torch.LongTensor)\n",
        "\n",
        "                    # Training\n",
        "                    train_multi(network, input_batch_10, target_batch_10, input_batch_20, target_batch_20, optimizer, epoch)\n",
        "\n",
        "                    # Evaluation every 5 epochs \n",
        "                    if epoch%5 == 0:\n",
        "                        # Initialization\n",
        "                        greedy = []\n",
        "                        beam = []\n",
        "                        shortest = []\n",
        "                        losses = []\n",
        "\n",
        "                        # Sub-selection of a part of the validation datasets\n",
        "                        sub_selection = np.arange(len(val_path_10))\n",
        "                        random.shuffle(sub_selection)\n",
        "                        sub_selection = sub_selection[:500]\n",
        "                        val_input_batch_10 = torch.Tensor(val_input_10[sub_selection])\n",
        "                        val_path_batch_10 = torch.Tensor(val_path_10[sub_selection])\n",
        "                        val_target_batch_10 = torch.Tensor(val_target_10[sub_selection])\n",
        "                        val_target_batch_10 = torch.flatten(val_target_batch_10, start_dim=0, end_dim=2)\n",
        "\n",
        "                        sub_selection = np.arange(len(val_path_10))\n",
        "                        random.shuffle(sub_selection)\n",
        "                        sub_selection = sub_selection[:500]\n",
        "                        val_input_batch_20 = torch.Tensor(val_input_20[sub_selection])\n",
        "                        val_path_batch_20 = torch.Tensor(val_path_20[sub_selection])\n",
        "                        val_target_batch_20 = torch.Tensor(val_target_20[sub_selection])\n",
        "                        val_target_batch_20 = torch.flatten(val_target_batch_20, start_dim=0, end_dim=2)\n",
        "\n",
        "                        # Loading of the current model into the cpu ones (because of RAM problem)\n",
        "                        model_val.load_state_dict(network.state_dict())\n",
        "\n",
        "                        for batch in range(1):\n",
        "                          # Evaluation on 10 nodes\n",
        "                          val_output_cpu_10 = model_val(val_input_batch_10[100*batch:100*(batch+1)])\n",
        "                          val_output_cpu_10 = F.log_softmax(val_output_cpu_10, dim=3)\n",
        "\n",
        "                          # Computation of the optimalities gap and of the loss on 10 nodes\n",
        "                          greedy.append(optimality_gap_greedy(val_input_batch_10[100*batch:100*(batch+1)],val_output_cpu_10, val_path_batch_10[100*batch:100*(batch+1)]))\n",
        "                          beam.append(optimality_gap_beamsearch(val_input_batch_10[100*batch:100*(batch+1)],val_output_cpu_10, val_path_batch_10[100*batch:100*(batch+1)]))\n",
        "                          shortest.append(optimality_gap_beamsearch_shortest(val_input_batch_10[100*batch:100*(batch+1)],val_output_cpu_10, val_path_batch_10[100*batch:100*(batch+1)]))\n",
        "                          val_output_cpu_10 = torch.flatten(val_output_cpu_10, start_dim=0, end_dim=2)\n",
        "                          losses.append(loss_function_10(val_output_cpu_10, val_target_batch_10[100*batch*10*10:100*(batch+1)*10*10]))\n",
        "\n",
        "                          # Evaluation on 20 nodes\n",
        "                          val_output_cpu_20 = model_val(val_input_batch_20[100*batch:100*(batch+1)])\n",
        "                          torch.set_printoptions(profile=\"full\")\n",
        "                          val_output_cpu_20 = F.log_softmax(val_output_cpu_20, dim=3)\n",
        "\n",
        "                          # Computation of the optimalities gap and of the loss on 20 nodes\n",
        "                          greedy.append(optimality_gap_greedy(val_input_batch_20[100*batch:100*(batch+1)],val_output_cpu_20, val_path_batch_20[100*batch:100*(batch+1)]))\n",
        "                          beam.append(optimality_gap_beamsearch(val_input_batch_20[100*batch:100*(batch+1)],val_output_cpu_20, val_path_batch_20[100*batch:100*(batch+1)]))\n",
        "                          shortest.append(optimality_gap_beamsearch_shortest(val_input_batch_20[100*batch:100*(batch+1)],val_output_cpu_20, val_path_batch_20[100*batch:100*(batch+1)]))\n",
        "                          val_output_cpu_20 = torch.flatten(val_output_cpu_20, start_dim=0, end_dim=2)\n",
        "                          losses.append(loss_function_20(val_output_cpu_20, val_target_batch_20[100*batch*20*20:100*(batch+1)*20*20]))\n",
        "\n",
        "                          gc.collect()  \n",
        "                          torch.cuda.empty_cache()\n",
        "\n",
        "                        # Saving of the optimalities gap and of the loss into files\n",
        "                        print(torch.tensor(greedy).mean(), file=f_greedy, flush=True)\n",
        "                        print(torch.tensor(beam).mean(), file=f_beam, flush=True)\n",
        "                        print(torch.tensor(shortest).mean(), file=f_shortest, flush=True)\n",
        "                        new_loss = torch.tensor(losses).mean()\n",
        "                        print(new_loss)\n",
        "                        print(new_loss, file=f_loss, flush=True)\n",
        "\n",
        "                        # Decreasing of the learning rate if the loss does not sufficiently decrease\n",
        "                        if (loss - new_loss) / loss < 0.01:\n",
        "                          lr /=1.01\n",
        "                          print('new lr : ',lr)\n",
        "                          optimizer = optim.Adam(network.parameters(), lr)\n",
        "                        loss = new_loss\n",
        "                    \n",
        "                    # Saving of the model every 20 epochs\n",
        "                    if (epoch%20 == 0):\n",
        "                        torch.save(network.state_dict(), 'model_Multi.pth')\n"
      ],
      "metadata": {
        "id": "OBQ4SlJTfPQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "ii9a7--MIUZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 nodes\n",
        "greedy = []\n",
        "beam = []\n",
        "shortest = []\n",
        "for batch in range(len(test_path_10)//100):\n",
        "    val_output = model_val(test_input_10[100*batch:100*(batch+1)])\n",
        "    val_output = F.log_softmax(val_output_cpu, dim=3)\n",
        "\n",
        "    greedy.append(optimality_gap_greedy(test_input_10[100*batch:100*(batch+1)],val_output_cpu, test_path_10[100*batch:100*(batch+1)]))\n",
        "    beam.append(optimality_gap_beamsearch(test_input_10[100*batch:100*(batch+1)],val_output_cpu, test_path_10[100*batch:100*(batch+1)]))\n",
        "    shortest.append(optimality_gap_beamsearch_shortest(test_input_10[100*batch:100*(batch+1)],val_output_cpu, test_path_10[100*batch:100*(batch+1)]))\n",
        "\n",
        "print(\"greedy \", torch.tensor(greedy).mean())\n",
        "print(\"beam \", torch.tensor(beam).mean())\n",
        "print(\"shortest \", torch.tensor(shortest).mean())"
      ],
      "metadata": {
        "id": "V8-YSus8KMyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 nodes\n",
        "greedy = []\n",
        "beam = []\n",
        "shortest = []\n",
        "for batch in range(len(test_path_20)//100):\n",
        "    val_output = model_val(test_input_20[100*batch:100*(batch+1)])\n",
        "    val_output = F.log_softmax(val_output_cpu, dim=3)\n",
        "\n",
        "    greedy.append(optimality_gap_greedy(test_input_20[100*batch:100*(batch+1)],val_output_cpu, test_path_20[100*batch:100*(batch+1)]))\n",
        "    beam.append(optimality_gap_beamsearch(test_input_20[100*batch:100*(batch+1)],val_output_cpu, test_path_20[100*batch:100*(batch+1)]))\n",
        "    shortest.append(optimality_gap_beamsearch_shortest(test_input_20[100*batch:100*(batch+1)],val_output_cpu, test_path_20[100*batch:100*(batch+1)]))\n",
        "\n",
        "print(\"greedy \", torch.tensor(greedy).mean())\n",
        "print(\"beam \", torch.tensor(beam).mean())\n",
        "print(\"shortest \", torch.tensor(shortest).mean())"
      ],
      "metadata": {
        "id": "vCAM4pKYMudZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "environment": {
      "kernel": "python3",
      "name": "pytorch-gpu.1-12.m102",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}